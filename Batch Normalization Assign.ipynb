{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91943dc1-7b69-4943-99dd-1084d6d08564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 14:20:03.597363: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-25 14:20:03.666091: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-25 14:20:03.666987: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-25 14:20:04.787038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "#After preprocessing, you can use the x_train, y_train, x_test, and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a15495-6980-43dd-bbe8-775059896d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b799765-73e9-40fa-be6d-9be8772c3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting jax>=0.3.15\n",
      "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.9.3)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.13)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518707 sha256=cb721a6b6f17824d0a44a487321d5948f47b669cbe6035f00d991a70d1246efc\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/4c/a3/e7/ea156aff3754a8f833f1b0c9587dec0bcfc9c551c439c9dcc7\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.20.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 jax-0.4.13 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c43d73-9fbb-4e13-8cba-7d8acfd748de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q. 2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,\n",
    "#Tensorlow, xyTorch)r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f282758e-a4cd-4fca-873c-2d96e4cbe3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.6613\n",
      "Epoch [200/1000], Loss: 0.6325\n",
      "Epoch [300/1000], Loss: 0.6133\n",
      "Epoch [400/1000], Loss: 0.5983\n",
      "Epoch [500/1000], Loss: 0.5858\n",
      "Epoch [600/1000], Loss: 0.5741\n",
      "Epoch [700/1000], Loss: 0.5628\n",
      "Epoch [800/1000], Loss: 0.5517\n",
      "Epoch [900/1000], Loss: 0.5404\n",
      "Epoch [1000/1000], Loss: 0.5291\n",
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the network architecture\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = 10\n",
    "hidden_size = 50\n",
    "output_size = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# Create the model\n",
    "model = FeedforwardNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Generate dummy input and target tensors (replace with your own data)\n",
    "input_data = torch.randn(100, input_size)\n",
    "target_data = torch.randint(output_size, (100,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, target_data)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Example prediction (replace with your own data)\n",
    "test_input = torch.randn(1, input_size)\n",
    "prediction = model(test_input)\n",
    "predicted_class = torch.argmax(prediction, dim=1)\n",
    "\n",
    "print(f'Predicted class: {predicted_class.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303eb321-cb7a-4d6b-814d-88276796fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93582 sha256=08c8272f0f9257133f1a0dc33411c2612a07d52c128767753a058eb48ab11f5e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1a/56/14/294a6c208bce35b0fc3170fe1049b2fd3f61ce6495fc3870b3\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.4 filelock-3.12.2 lit-16.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a23ba3-9555-4b66-b26e-fbc2efe737d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. Train the neural network on the chosen dataset without using batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d3942bc-a980-4738-ba18-dc8f8e4726d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Explain the concept of batch normalization in the context of Artificial Neural \n",
    "#Networksr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "868a6d88-c9d3-471a-bc03-1e9c77cb9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch normalization is a technique used in Artificial Neural Networks (ANNs) to improve\n",
    "#the training process and performance of the network. It addresses the issue of internal \n",
    "#covariate shift, which refers to the change in the distribution of the network's input\n",
    "#data as it propagates through the layers during training.\n",
    "\n",
    "#The basic idea behind batch normalization is to normalize the inputs of each layer by \n",
    "#adjusting and scaling them so that they have zero mean and unit variance. This \n",
    "#normalization is performed over mini-batches of training examples rather than \n",
    "#individual examples. The normalization process involves the following steps:\n",
    "\n",
    "#Computing the mean and variance: For each feature in the mini-batch, the mean and \n",
    "#variance are calculated.\n",
    "\n",
    "#Normalization: The input values of each feature are normalized by subtracting the mean \n",
    "#and dividing by the square root of the variance. This step ensures that the features \n",
    "#have zero mean and unit variance.\n",
    "\n",
    "#Scale and shift: After normalization, the features are scaled and shifted by learnable \n",
    "#parameters called gamma and beta, respectively. These parameters allow the network to \n",
    "#learn the optimal scale and shift for each feature, which helps in preserving the \n",
    "#representation capacity of the network.\n",
    "\n",
    "#Batch normalization provides several benefits in the training process of ANNs:\n",
    "\n",
    "#Accelerated training: By normalizing the inputs, batch normalization helps in reducing \n",
    "#the internal covariate shift problem, which can lead to faster convergence during \n",
    "#training. This allows the network to learn more quickly and reduces the number of \n",
    "#epochs required for training.\n",
    "\n",
    "#Improved gradient flow: Normalizing the inputs of each layer helps in maintaining a\n",
    "#more stable gradient flow throughout the network. This reduces the likelihood of \n",
    "#vanishing or exploding gradients, which can hinder the training process.\n",
    "\n",
    "#Regularization effect: Batch normalization acts as a form of regularization by adding \n",
    "#some noise to the network during training. This noise helps in reducing overfitting \n",
    "#and can improve the generalization performance of the network on unseen data.\n",
    "\n",
    "#Network robustness: Batch normalization makes the network less sensitive to the \n",
    "#initialization of weights, as the normalization process reduces the impact of scale \n",
    "#and shift differences between layers.\n",
    "\n",
    "#Overall, batch normalization is a powerful technique that can significantly improve the\n",
    "#training dynamics and generalization performance of Artificial Neural Networks. It has \n",
    "#become a common practice in deep learning architectures and has contributed to the \n",
    "#success of various state-of-the-art models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "251b84e1-4453-495a-b1ce-cc34ebae0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Describe the benefits of using batch normalization during trainingr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ad75aed-aefb-4613-9d46-2d1136ab3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch normalization is a technique commonly used in deep learning models during the \n",
    "#training process. It aims to stabilize and improve the training process by normalizing \n",
    "#the intermediate activations within each mini-batch. Here are several benefits of using \n",
    "#batch normalization:\n",
    "\n",
    "#Improved training speed: Batch normalization significantly accelerates the training \n",
    "#process by reducing the number of training iterations required to converge. It helps in\n",
    "#avoiding the saturation of activation functions by ensuring that the inputs to each \n",
    "#layer remain within a reasonable range. This allows for higher learning rates and faster\n",
    "#convergence.\n",
    "\n",
    "#Increased stability and robustness: By normalizing the activations, batch normalization\n",
    "#reduces the problem of covariate shift. Covariate shift occurs when the distribution of\n",
    "#the input to a layer changes during training, which can make learning difficult. \n",
    "#Batch normalization mitigates this issue by ensuring that the mean and variance of each\n",
    "#mini-batch are normalized, leading to more stable and consistent gradients.\n",
    "\n",
    "#Reduced sensitivity to weight initialization: Batch normalization reduces the \n",
    "#dependence of a model on the initial choice of weights. It helps in overcoming the \n",
    "#vanishing or exploding gradients problem that often occurs in deep networks. With batch \n",
    "#normalization, the network becomes less sensitive to the scale of weight initialization,\n",
    "#making it easier to train deep architectures effectively.\n",
    "\n",
    "#Regularization effect: Batch normalization introduces a slight regularization effect \n",
    "#by adding noise to the network's activations. This noise acts as a form of \n",
    "#regularization and helps prevent overfitting. It reduces the need for other \n",
    "#regularization techniques, such as dropout or weight decay, although they can still \n",
    "#be used in conjunction with batch normalization for even better performance.\n",
    "\n",
    "#Improved generalization: Batch normalization has been shown to improve the \n",
    "#generalization capabilities of deep learning models. By reducing internal covariate \n",
    "#shift and stabilizing the network's activations, it encourages the network to learn \n",
    "#more meaningful features that generalize well to unseen data. This can lead to better \n",
    "#performance on validation and test sets.\n",
    "\n",
    "#Enabling higher learning rates: Batch normalization enables the use of higher learning\n",
    "#rates during training. With normalized activations, the network is less likely to \n",
    "#encounter exploding or vanishing gradients, allowing for larger step sizes in the \n",
    "#weight update process. This accelerates the convergence process and helps the model \n",
    "#find better optima in the parameter space.\n",
    "\n",
    "#In summary, batch normalization provides numerous benefits during training, including \n",
    "#improved training speed, increased stability and robustness, reduced sensitivity to \n",
    "#weight initialization, regularization effects, improved generalization, and the ability\n",
    "#to use higher learning rates. It has become a standard component in deep learning \n",
    "#architectures, contributing to their effectiveness and widespread adoption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afcdcb23-a0ba-4e9b-b5ee-fdc26d2060fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. Discuss the working principle of batch normalization, including the normalization \n",
    "#step and the learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d03070-edc2-409a-a8d5-91431b7967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization is a technique commonly used in deep learning models to improve \n",
    "#training speed and stability. It normalizes the inputs of each layer in a neural \n",
    "#network by adjusting and scaling the activations. The main idea behind batch \n",
    "#normalization is to reduce the internal covariate shift, which refers to the change in\n",
    "#the distribution of layer inputs during training.\n",
    "\n",
    "#The working principle of batch normalization involves two key steps: normalization and \n",
    "#learnable parameters.\n",
    "\n",
    "#Normalization Step:\n",
    "#During the training process, batch normalization normalizes the inputs of each layer by \n",
    "#subtracting the batch mean and dividing by the batch standard deviation. The \n",
    "#normalization step is performed on a mini-batch of samples within each training \n",
    "#iteration. The normalization formula can be represented as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f215e752-1bc1-4b06-9f4c-05dc93cefafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Experiment with different batch sizes and observe the effect on the training \n",
    "#dynamics and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6507369-4500-4f98-8cea-ba4ddce292fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When training a machine learning model, the batch size refers to the number of training \n",
    "#examples used in each iteration of the training algorithm. Experimenting with different\n",
    "#batch sizes can have an impact on the training dynamics and model performance. \n",
    "#Here are some observations you may encounter:\n",
    "\n",
    "#Computational Efficiency: Larger batch sizes tend to utilize hardware resources more \n",
    "#efficiently. When processing a batch, parallelism can be exploited, leading to faster \n",
    "#training times. This is particularly advantageous when using hardware accelerators like\n",
    "#GPUs.\n",
    "\n",
    "#Generalization: Smaller batch sizes often lead to better generalization. By exposing \n",
    "#the model to a variety of individual examples in each batch, it can learn more diverse \n",
    "#patterns and become less prone to overfitting. However, this may also depend on other \n",
    "#factors like the complexity of the model architecture and the size of the dataset.\n",
    "\n",
    "#Noise in Gradient Estimation: Larger batch sizes provide a more accurate estimate of \n",
    "#the true gradient compared to smaller batch sizes. Smaller batches introduce more \n",
    "#randomness, as they represent only a subset of the entire training data. This noise can\n",
    "#help the model escape sharp local minima, but it can also hinder convergence and \n",
    "#stability.\n",
    "\n",
    "#Convergence Speed: Larger batch sizes often result in faster convergence during \n",
    "#training. As larger batches provide a more stable gradient estimate, the optimization \n",
    "#process may require fewer iterations to reach a satisfactory solution. On the other \n",
    "#hand, smaller batch sizes may slow down convergence due to the noise and increased \n",
    "#number of iterations required.\n",
    "\n",
    "#Memory Requirements: Larger batch sizes consume more memory during training, as the \n",
    "#gradients for each batch need to be stored until the update step. If the available \n",
    "#memory is limited, using smaller batch sizes might be necessary to fit the training \n",
    "#process within the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfe9a3bc-f943-4764-9a0e-9fa6647209c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "#neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c5e8b-5ced-4e9f-b829-82e5fdb2c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch normalization is a widely used technique in training neural networks that aims to \n",
    "#improve the learning process and convergence of the model. It operates by normalizing\n",
    "#the inputs to each layer of the neural network based on the statistics of a batch of \n",
    "#training examples. While batch normalization offers several advantages, it also has \n",
    "#some potential limitations. Let's discuss both aspects:\n",
    "\n",
    "#Advantages of Batch Normalization:\n",
    "\n",
    "#Improved Training Speed: Batch normalization reduces the internal covariate shift,\n",
    "#which refers to the change in the distribution of layer inputs during training. By \n",
    "#normalizing the inputs, it ensures that each layer receives inputs with a stable \n",
    "#distribution, leading to faster convergence and reduced training time.\n",
    "\n",
    "#Enhanced Gradient Flow: Normalizing the inputs helps in maintaining a more consistent \n",
    "#scale for the activation values in each layer. This aids in propagating gradients \n",
    "#effectively during backpropagation, allowing for more stable and efficient training. \n",
    "#It mitigates the vanishing/exploding gradient problems and enables the use of higher \n",
    "#learning rates.\n",
    "\n",
    "#Increased Robustness to Parameter Initialization: Batch normalization reduces the \n",
    "#sensitivity of neural networks to the choice of initial parameter values. It helps to \n",
    "#stabilize the network's behavior, making it less dependent on the specific \n",
    "#initialization scheme and allowing for faster convergence.\n",
    "\n",
    "#Regularization Effect: Batch normalization acts as a form of regularization by adding a \n",
    "#small amount of noise to the layer inputs. This noise contributes to the model's \n",
    "#robustness and reduces overfitting, allowing for better generalization to unseen data.\n",
    "\n",
    "#Network Scalability: Batch normalization makes it easier to train deeper neural \n",
    "#networks. It helps alleviate the issues of gradient vanishing or exploding, which \n",
    "#are more likely to occur in deep architectures. This scalability is crucial for \n",
    "#the success of modern deep learning models.\n",
    "\n",
    "#Limitations of Batch Normalization:\n",
    "\n",
    "#Batch Size Dependency: The effectiveness of batch normalization relies on having a \n",
    "#sufficiently large batch size. Smaller batch sizes may result in inaccurate estimates of batch statistics, leading to suboptimal normalization. Consequently, the benefits of batch normalization may diminish when using small batch sizes or when performing online/incremental learning.\n",
    "\n",
    "Inference Mode Differences: During inference, when making predictions on individual examples or small batches, the statistics used for normalization (mean and variance) are typically based on the entire training dataset or a moving average collected during training. This discrepancy between training and inference modes can introduce a mismatch and affect the model's performance during deployment.\n",
    "\n",
    "Computational Overhead: Batch normalization introduces additional computations during training, as it requires calculating and applying normalization transformations to the inputs of each layer. This overhead becomes more significant for large-scale models or when training on resource-constrained devices, impacting the overall training time and memory requirements.\n",
    "\n",
    "Sensitivity to Learning Rate: Batch normalization can make neural networks more sensitive to the choice of learning rate. In some cases, using high learning rates with batch normalization can lead to unstable training or divergence. Careful tuning of the learning rate and its schedule becomes crucial to achieve optimal performance.\n",
    "\n",
    "Constraint on Network Architectures: Batch normalization assumes the layer inputs have a meaningful and consistent distribution across the training data. However, for certain types of architectures, such as recurrent neural networks (RNNs) with varying sequence lengths or generative models with discrete outputs, applying batch normalization directly may not be appropriate or may require adaptations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
